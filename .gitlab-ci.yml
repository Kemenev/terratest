stages:
  - validate
  - plan
  - repair
  - apply

variables:
  TF_BACKEND_ADDRESS: "$CI_API_V4_URL/projects/$CI_PROJECT_ID/terraform/state/$CI_COMMIT_REF_NAME"
  TF_BACKEND_LOCK: "$CI_API_V4_URL/projects/$CI_PROJECT_ID/terraform/state/$CI_COMMIT_REF_NAME/lock"
  TF_VAR_tf_env: $CI_COMMIT_REF_NAME

.tf_job: &tf_job
  image: alpine/terragrunt:1.12.2
  tags:stages:
  - validate
  - plan
  - repair
  - apply

variables:
  TF_BACKEND_ADDRESS: "$CI_API_V4_URL/projects/$CI_PROJECT_ID/terraform/state/$CI_COMMIT_REF_NAME"
  TF_BACKEND_LOCK: "$CI_API_V4_URL/projects/$CI_PROJECT_ID/terraform/state/$CI_COMMIT_REF_NAME/lock"
  TF_VAR_tf_env: $CI_COMMIT_REF_NAME

.tf_job: &tf_job
  image: alpine/terragrunt:1.12.2
  tags:
    - docker
  before_script:
    - unzip -o terraform-plugins.zip
    - export TF_VAR_vsphere_password=$(echo "$TF_VAR_vsphere_password" | base64 -d)
    - mkdir -p ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64
    - mkdir -p ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64
    - cp terraform-plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/terraform-provider-vsphere_v2.15.0 ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64
    - cp terraform-plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64/terraform-provider-netbox_v5.0.0 ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64
    - chmod +x ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64/terraform-provider-vsphere_v2.15.0
    - chmod +x ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64/terraform-provider-netbox_v5.0.0
    - echo "$NETBOX_CERT" > /usr/local/share/ca-certificates/netbox.crt
    - update-ca-certificates
    - terraform init
      -backend-config="address=$TF_BACKEND_ADDRESS"
      -backend-config="username=gitlab-ci-token"
      -backend-config="password=$CI_JOB_TOKEN"

validate:
  <<: *tf_job
  stage: validate
  script:
    - terraform validate

plan:
  <<: *tf_job
  stage: plan
  # plan может "упасть" (guard или terraform error),
  # но pipeline должен продолжиться, чтобы были manual кнопки.
  allow_failure: true
  script:
    # Всегда создаём файлы, чтобы artifacts не ломались
    - touch tfplan.txt state_before.txt vm_creates_or_replaces.txt missing_vm_addresses.txt missing_vm_names.txt

    # Снимок state ДО plan (нужно, чтобы отличать "новые" VM от пересоздания)
    - terraform state list > state_before.txt || true

    # План (может упасть по любой причине)
    - terraform plan -out=tfplan
    - terraform show -no-color tfplan > tfplan.txt || true

    # Guard: если Terraform хочет создать/заменить VM, которая уже была в state — останавливаемся.
    - |
      grep 'vsphere_virtual_machine' tfplan.txt | grep -E 'will be created|will be replaced|must be replaced|destroyed and then created' \
        | sed -E 's/^.*# *//; s/ will be created$//; s/ will be replaced$//; s/ must be replaced$//; s/ destroyed and then created$//' \
        | sort -u > vm_creates_or_replaces.txt || true

      # Пересечение с state_before: это "пересоздание" (обычно VM удалили вручную)
      grep -Fx -f state_before.txt vm_creates_or_replaces.txt > missing_vm_addresses.txt || true

      # Для удобства — просто имена VM
      sed -n 's/.*\["\([^"]\+\)"\].*/\1/p' missing_vm_addresses.txt | sort -u > missing_vm_names.txt || true

      if [ -s missing_vm_addresses.txt ]; then
        echo "ERROR: Обнаружены VM, которые Terraform пытается пересоздать (скорее всего удалены вручную):"
        cat missing_vm_names.txt
        echo ""
        echo "Дальше:"
        echo " - если VM НЕ нужна -> убери ее из YAML/конфига и запусти manual job: retire_missing"
        echo " - если VM нужна -> запусти manual job: recreate_missing"
        exit 1
      fi
  artifacts:
    when: always
    paths:
      - tfplan
      - tfplan.txt
      - state_before.txt
      - vm_creates_or_replaces.txt
      - missing_vm_addresses.txt
      - missing_vm_names.txt
      - .terraform.lock.hcl
      - .terraform
      - tf-debug.log
    expire_in: 1 day

retire_missing:
  <<: *tf_job
  stage: repair
  when: manual
  allow_failure: false
  dependencies:
    - plan
  script:
    - |
      if [ ! -s missing_vm_addresses.txt ]; then
        echo "Нет missing VM для retire (missing_vm_addresses.txt пуст)."
        exit 0
      fi

      echo "Retire: удаляем из Terraform state ресурсы vSphere VM (объекты в vCenter НЕ трогаем):"
      cat missing_vm_addresses.txt

      while read -r addr; do
        [ -z "$addr" ] && continue
        terraform state rm "$addr" || true
      done < missing_vm_addresses.txt

      echo ""
      echo "Retire: отвязываем NetBox ресурсы из state (в NetBox ничего НЕ удаляется):"

      # Имена VM
      touch retire_vm_names.txt retire_netbox_state_rm.txt
      sed -n 's/.*\["\([^"]\+\)"\].*/\1/p' missing_vm_addresses.txt | sort -u > retire_vm_names.txt || true

      if [ ! -s retire_vm_names.txt ]; then
        echo "Не удалось извлечь имена VM — пропускаем NetBox часть."
        exit 0
      fi

      echo "VM:"
      cat retire_vm_names.txt

      : > retire_netbox_state_rm.txt
      while read -r vm; do
        [ -z "$vm" ] && continue
        terraform state list | grep '^netbox_' | grep -F "[\"$vm\"]" >> retire_netbox_state_rm.txt || true
      done < retire_vm_names.txt

      sort -u retire_netbox_state_rm.txt -o retire_netbox_state_rm.txt

      if [ -s retire_netbox_state_rm.txt ]; then
        echo ""
        echo "Удаляем из state (NetBox объекты останутся):"
        cat retire_netbox_state_rm.txt

        while read -r nb; do
          [ -z "$nb" ] && continue
          terraform state rm "$nb" || true
        done < retire_netbox_state_rm.txt
      else
        echo "NetBox ресурсов для этих VM в state не найдено."
      fi

      echo ""
      echo "Готово."
      echo "Важно: после retire VM должна быть убрана из YAML/конфига, иначе Terraform может создать её снова."
  artifacts:
    when: always
    paths:
      - retire_vm_names.txt
      - retire_netbox_state_rm.txt
    expire_in: 1 day

recreate_missing:
  <<: *tf_job
  stage: apply
  when: manual
  allow_failure: false
  dependencies:
    - plan
  script:
    - |
      if [ ! -s missing_vm_names.txt ]; then
        echo "Нет missing VM для восстановления."
        exit 0
      fi

      echo "Восстанавливаем missing VM. Важно: НЕ используем tfplan, потому что plan мог упасть."
      echo "VM:"
      cat missing_vm_names.txt

      # Новый apply пересоберёт план по текущему состоянию
      terraform apply -auto-approve

apply:
  <<: *tf_job
  stage: apply
  when: manual
  allow_failure: false
  dependencies:
    - plan
  script:
    - |
      # Если guard нашел missing VM — обычный apply запрещен
      if [ -s missing_vm_addresses.txt ]; then
        echo "ERROR: Обнаружены missing VM."
        echo "Используй manual job recreate_missing (восстановить) или retire_missing (удалить навсегда)."
        cat missing_vm_names.txt || true
        exit 1
      fi

      # Если по какой-то причине tfplan нет — не можем применить
      if [ ! -f tfplan ]; then
        echo "ERROR: tfplan не найден. Plan не был успешно создан, apply невозможен."
        exit 1
      fi

      terraform apply -auto-approve tfplan

    - docker
  before_script:
    - unzip -o terraform-plugins.zip
    - export TF_VAR_vsphere_password=$(echo "$TF_VAR_vsphere_password" | base64 -d)
    - mkdir -p ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64
    - mkdir -p ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64
    - cp terraform-plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/terraform-provider-vsphere_v2.15.0 ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64
    - cp terraform-plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64/terraform-provider-netbox_v5.0.0 ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64
    - chmod +x ~/.terraform.d/plugins/registry.terraform.io/hashicorp/vsphere/2.15.0/linux_amd64/terraform-provider-vsphere_v2.15.0
    - chmod +x ~/.terraform.d/plugins/registry.terraform.io/e-breuninger/netbox/5.0.0/linux_amd64/terraform-provider-netbox_v5.0.0
    - echo "$NETBOX_CERT" > /usr/local/share/ca-certificates/netbox.crt
    - update-ca-certificates
    - terraform init
      -backend-config="address=$TF_BACKEND_ADDRESS"
      -backend-config="username=gitlab-ci-token"
      -backend-config="password=$CI_JOB_TOKEN"

validate:
  <<: *tf_job
  stage: validate
  script:
    - terraform validate

plan:
  <<: *tf_job
  stage: plan
  # План может "упасть" из-за Guard — но pipeline должен продолжиться,
  # чтобы админ мог нажать manual jobs (retire/recreate).
  allow_failure: true
  script:
    # Всегда создаем файлы, чтобы artifacts не ломались
    - : > tfplan.txt
    - : > state_before.txt
    - : > vm_creates_or_replaces.txt
    - : > missing_vm_addresses.txt
    - : > missing_vm_names.txt

    # Снимок state ДО plan (нужно, чтобы отличать "новые" VM от пересоздания)
    - terraform state list > state_before.txt || true

    - terraform plan -out=tfplan
    - terraform show -no-color tfplan > tfplan.txt || true

    # Guard: если Terraform хочет создать/заменить VM, которая уже была в state — останавливаемся.
    - |
      grep 'vsphere_virtual_machine' tfplan.txt | grep -E 'will be created|will be replaced|must be replaced|destroyed and then created' \
        | sed -E 's/^.*# *//; s/ will be created$//; s/ will be replaced$//; s/ must be replaced$//; s/ destroyed and then created$//' \
        | sort -u > vm_creates_or_replaces.txt || true

      # Пересечение с state_before: это "пересоздание" (обычно VM удалили вручную)
      grep -Fx -f state_before.txt vm_creates_or_replaces.txt > missing_vm_addresses.txt || true

      # Для удобства — просто имена VM
      sed -n 's/.*\["\([^"]\+\)"\].*/\1/p' missing_vm_addresses.txt | sort -u > missing_vm_names.txt || true

      if [ -s missing_vm_addresses.txt ]; then
        echo "ERROR: Обнаружены VM, которые Terraform пытается пересоздать (скорее всего удалены вручную):"
        cat missing_vm_names.txt
        echo ""
        echo "Дальше:"
        echo " - если VM НЕ нужна -> убери ее из YAML/конфига и запусти manual job: retire_missing"
        echo " - если VM нужна -> запусти manual job: recreate_missing"
        exit 1
      fi
  artifacts:
    when: always
    paths:
      - tfplan
      - tfplan.txt
      - state_before.txt
      - vm_creates_or_replaces.txt
      - missing_vm_addresses.txt
      - missing_vm_names.txt
      - .terraform.lock.hcl
      - .terraform
      - tf-debug.log
    expire_in: 1 day

retire_missing:
  <<: *tf_job
  stage: repair
  when: manual
  allow_failure: false
  dependencies:
    - plan
  script:
    - |
      if [ ! -s missing_vm_addresses.txt ]; then
        echo "Нет missing VM для удаления из state (missing_vm_addresses.txt пуст)."
        exit 0
      fi

      echo "Удаляем из Terraform state следующие ресурсы (обычно vSphere VM):"
      cat missing_vm_addresses.txt

      while read -r addr; do
        [ -z "$addr" ] && continue
        terraform state rm "$addr"
      done < missing_vm_addresses.txt

      echo ""
      echo "Теперь отвязываем NetBox ресурсы из state (в NetBox ничего НЕ удаляется):"

      # 1) достаём имена VM из адресов вида ...["NAME"]
      : > retire_vm_names.txt
      sed -n 's/.*\["\([^"]\+\)"\].*/\1/p' missing_vm_addresses.txt | sort -u > retire_vm_names.txt || true

      if [ ! -s retire_vm_names.txt ]; then
        echo "Не удалось извлечь имена VM из missing_vm_addresses.txt — пропускаем NetBox часть."
        exit 0
      fi

      echo "VM:"
      cat retire_vm_names.txt

      # 2) собираем список netbox_* адресов для удаления из state
      : > retire_netbox_state_rm.txt

      while read -r vm; do
        [ -z "$vm" ] && continue
        terraform state list | grep '^netbox_' | grep -F "[\"$vm\"]" >> retire_netbox_state_rm.txt || true
      done < retire_vm_names.txt

      sort -u retire_netbox_state_rm.txt -o retire_netbox_state_rm.txt

      if [ -s retire_netbox_state_rm.txt ]; then
        echo ""
        echo "Удаляем из state (NetBox объекты останутся):"
        cat retire_netbox_state_rm.txt

        while read -r nb; do
          [ -z "$nb" ] && continue
          terraform state rm "$nb"
        done < retire_netbox_state_rm.txt
      else
        echo "NetBox ресурсов для этих VM в state не найдено."
      fi

      echo ""
      echo "Готово."
      echo "Важно: убери эти VM из YAML/конфига (через MR), иначе Terraform попытается создать их снова."
  artifacts:
    when: always
    paths:
      - retire_vm_names.txt
      - retire_netbox_state_rm.txt
    expire_in: 1 day

recreate_missing:
  <<: *tf_job
  stage: apply
  when: manual
  allow_failure: false
  dependencies:
    - plan
  script:
    - |
      if [ ! -s missing_vm_addresses.txt ]; then
        echo "missing_vm_addresses.txt пуст — нечего пересоздавать. Используй обычный apply."
        exit 0
      fi

      echo "Пересоздаем missing VM согласно tfplan:"
      cat missing_vm_names.txt || true
      terraform apply -auto-approve tfplan

apply:
  <<: *tf_job
  stage: apply
  script:
    - |
      # Защита от случайного пересоздания: если Guard нашел missing VM,
      # используй recreate_missing (восстановить) или retire_missing (удалить навсегда).
      if [ -s missing_vm_addresses.txt ]; then
        echo "ERROR: Обнаружены missing VM."
        echo "Используй manual job recreate_missing (восстановить) или retire_missing (удалить из state после удаления из YAML)."
        cat missing_vm_names.txt || true
        exit 1
      fi

      terraform apply -auto-approve tfplan
  when: manual
  dependencies:
    - plan
  allow_failure: false
#


